<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PersonaAI - Digital Avatar with RAG | <b>Kunle S. Oguntoye</b> </title> <meta name="author" content="Kunle Sunday Oguntoye"> <meta name="description" content="Personalized AI assistant that mimics individual personality using Retrieval-Augmented Generation and Llama 2 70B"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kunle-xy.github.io/projects/8_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <b>Kunle S. Oguntoye</b> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PersonaAI - Digital Avatar with RAG</h1> <p class="post-description">Personalized AI assistant that mimics individual personality using Retrieval-Augmented Generation and Llama 2 70B</p> </header> <article> <h2 id="the-challenge-creating-authentic-digital-personalities">The Challenge: Creating Authentic Digital Personalities</h2> <p>Traditional large language models provide generic responses that lack personal context. Training custom models from scratch to capture individual personalities is computationally expensive, resource-intensive, and raises significant privacy concerns. The fundamental question becomes: <strong>How do we create AI systems that authentically represent individual personalities while remaining efficient, scalable, and privacy-preserving?</strong></p> <p>This challenge motivated the development of <strong>PersonaAI</strong>, a system that leverages Retrieval-Augmented Generation (RAG) combined with prompt-engineered Llama 2 70B to create highly personalized digital avatars‚Äîoffering a lightweight, sustainable alternative to traditional large language model training methods.</p> <hr> <h2 id="the-solution-rag-powered-personalization">The Solution: RAG-Powered Personalization</h2> <p>PersonaAI combines three core technologies to deliver personalized AI interactions:</p> <ol> <li> <strong>Retrieval-Augmented Generation (RAG)</strong> - Dynamically retrieves relevant personal context from user documents</li> <li> <strong>Llama 2 70B Language Model</strong> - Open-source foundation model deployed via Replicate API with dual-template prompt engineering</li> <li> <strong>Vector Similarity Search</strong> - Cosine similarity-based retrieval (top-2) to find contextually relevant information</li> </ol> <hr> <h2 id="how-personaai-works">How PersonaAI Works</h2> <h3 id="stage-1-document-ingestion--embedding">Stage 1: Document Ingestion &amp; Embedding</h3> <p>Users upload text documents (messages, emails, journal entries, social media posts) that represent their communication style and personality. The system processes these documents through several steps:</p> <p><strong>Text Processing Pipeline:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pseudocode illustration
</span><span class="n">documents</span> <span class="o">=</span> <span class="nf">upload_user_documents</span><span class="p">()</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="nf">split_into_chunks</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># Generate embeddings for each chunk
</span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="nf">generate_embedding</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">store_in_database</span><span class="p">({</span>
        <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">chunk</span><span class="p">.</span><span class="n">text</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">topic</span><span class="sh">'</span><span class="p">:</span> <span class="nf">extract_topic</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">text</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">vector</span><span class="sh">'</span><span class="p">:</span> <span class="n">embedding</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">:</span> <span class="n">chunk</span><span class="p">.</span><span class="n">uploaded_at</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">user_id</span><span class="sh">'</span><span class="p">:</span> <span class="n">current_user</span><span class="p">.</span><span class="nb">id</span>
    <span class="p">})</span>
</code></pre></div></div> <p><strong>Key Implementation Details:</strong></p> <ul> <li>Documents are chunked into 512-token segments to fit embedding model context windows</li> <li>Topic extraction identifies thematic clusters for improved retrieval</li> <li>Vector embeddings capture semantic meaning of each text segment</li> <li>All data is stored with user-specific isolation for privacy</li> </ul> <h3 id="stage-2-query-processing-with-cosine-similarity">Stage 2: Query Processing with Cosine Similarity</h3> <p>When a user submits a prompt, PersonaAI retrieves the most relevant personal context using <strong>cosine similarity</strong>:</p> <p><strong>Why Cosine Similarity?</strong></p> <p>Cosine similarity measures the angle between two vectors in high-dimensional space, making it ideal for comparing semantic meaning regardless of document length:</p> \[\text{cosine\_similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}\] <p><strong>Properties that make it perfect for RAG:</strong></p> <ul> <li> <strong>Scale-invariant</strong> - Focuses on direction, not magnitude (long vs short documents treated fairly)</li> <li> <strong>Normalized to [‚àí1, 1]</strong> - Easy threshold setting for relevance filtering</li> <li> <strong>Computationally efficient</strong> - Fast dot product and norm calculations</li> <li> <strong>Semantically meaningful</strong> - High cosine similarity = high semantic similarity</li> </ul> <p><strong>Retrieval Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate embedding for user query
</span><span class="n">query_embedding</span> <span class="o">=</span> <span class="nf">generate_embedding</span><span class="p">(</span><span class="n">user_prompt</span><span class="p">)</span>

<span class="c1"># Compute cosine similarity with all stored documents
</span><span class="n">similarities</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">user_documents</span><span class="p">:</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">vector</span><span class="p">)</span>
    <span class="n">similarities</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">similarity</span><span class="p">,</span> <span class="n">doc</span><span class="p">))</span>

<span class="c1"># Sort by similarity and retrieve top-k
</span><span class="n">similarities</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">top_k_docs</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># Default k=2
</span></code></pre></div></div> <h3 id="stage-3-top-k-retrieval-strategy">Stage 3: Top-K Retrieval Strategy</h3> <p><strong>What is Top-K Retrieval?</strong></p> <p>Top-k retrieval selects the <strong>k most relevant documents</strong> based on similarity scores. This is crucial for balancing context quality and computational efficiency.</p> <p><strong>Why K Matters:</strong></p> <table> <thead> <tr> <th>K Value</th> <th>Trade-offs</th> </tr> </thead> <tbody> <tr> <td><strong>k = 1</strong></td> <td>Fastest, but may miss important context from slightly less similar documents</td> </tr> <tr> <td><strong>k = 2</strong></td> <td> <strong>Optimal balance</strong> - Minimal noise while capturing primary context</td> </tr> <tr> <td><strong>k = 3-5</strong></td> <td>More comprehensive context, but risks introducing less relevant information</td> </tr> <tr> <td><strong>k = 10+</strong></td> <td>Too much noise, increases token costs and inference latency significantly</td> </tr> </tbody> </table> <p>In PersonaAI, <strong>k = 2</strong> provides the sweet spot:</p> <ul> <li>Focuses on the most relevant context without noise</li> <li>Minimizes token usage for faster inference</li> <li>Keeps prompts concise and focused</li> <li>Reduces computational overhead for similarity search</li> <li>Works well with Llama 2‚Äôs context window</li> </ul> <p><strong>Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate cosine similarity for all documents
</span><span class="n">similarities</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">user_documents</span><span class="p">:</span>
    <span class="c1"># Dot product normalized by magnitudes
</span>    <span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">vector</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">vector</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">similarities</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">similarity</span><span class="p">,</span> <span class="n">doc</span><span class="p">))</span>

<span class="c1"># Sort and retrieve top-2
</span><span class="n">similarities</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">top_2_docs</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Use top-2 as context for prompt
</span><span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">doc</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">top_2_docs</span><span class="p">])</span>
</code></pre></div></div> <hr> <h2 id="prompt-engineering-the-secret-sauce">Prompt Engineering: The Secret Sauce</h2> <p>PersonaAI‚Äôs effectiveness comes from carefully engineered prompts that adapt based on whether user documents are available. The implementation uses <strong>two distinct prompt templates</strong> to handle different scenarios.</p> <h3 id="system-prompt-global-instruction">System Prompt (Global Instruction)</h3> <p>All interactions begin with this system-level instruction sent to Llama 2:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are a helpful and truthful assistant, answer only if you know the answer.
If you do not know the answer, truthfully say 'I do not know'.
</code></pre></div></div> <p>This establishes the model‚Äôs core behavior: <strong>honesty over hallucination</strong>.</p> <h3 id="prompt-template-1-empty-database-cold-start">Prompt Template 1: Empty Database (Cold Start)</h3> <p>When a user has <strong>not yet uploaded any documents</strong>, PersonaAI uses this template:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Respond to the QUESTION below:
- If the QUESTION is a general greeting or an inquiry about personal welfare
  (e.g., </span><span class="sh">"</span><span class="s">How are you?</span><span class="sh">"</span><span class="s"> or </span><span class="sh">"</span><span class="s">Good day</span><span class="sh">"</span><span class="s">), reply in a friendly and jovial manner.
  These responses should be warm and engaging.

- If the QUESTION is too specific and lacks the necessary context or details
  for a comprehensive answer, kindly request that the user provide more specific
  details or context to enable a more accurate response.

- If the QUESTION can be answered with general knowledge and the answer is known,
  provide a generalized, honest, and harmless answer.

- If you are unable to answer the QUESTION due to a lack of information, either
  from the context provided or within general knowledge parameters, clearly state
  </span><span class="sh">"</span><span class="s">I DO NOT KNOW</span><span class="sh">"</span><span class="s">.

QUESTION:
{question}

ANSWER:
</span><span class="sh">"""</span>
</code></pre></div></div> <p><strong>Key Design Principles:</strong></p> <ol> <li> <strong>Graceful Greeting Handling</strong> - Responds warmly to casual interactions without requiring context</li> <li> <strong>Clarification Requests</strong> - Proactively asks for more details when questions are vague</li> <li> <strong>General Knowledge Fallback</strong> - Leverages Llama 2‚Äôs base knowledge when appropriate</li> <li> <strong>Explicit Uncertainty</strong> - Forces the model to admit ignorance rather than fabricate answers</li> </ol> <h3 id="prompt-template-2-with-context-rag-mode">Prompt Template 2: With Context (RAG Mode)</h3> <p>When the user <strong>has uploaded documents</strong>, the top-2 retrieved documents are injected as context:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Respond to the QUESTION below:
- If the QUESTION is a general greeting or an inquiry about welfare
  (e.g., </span><span class="sh">"</span><span class="s">How are you?</span><span class="sh">"</span><span class="s"> or </span><span class="sh">"</span><span class="s">Good day</span><span class="sh">"</span><span class="s">), reply in a friendly and jovial manner.
  Do not include the CONTEXT in your response.

- If the QUESTION requires specific information from the CONTEXT (provided below)
  and the answer can be determined from the CONTEXT, provide that answer.

- If the QUESTION pertains to general knowledge or topics not covered in the
  CONTEXT, such as current events or public information, and if this information
  is readily available to the model, provide an informed response using general
  knowledge.

- If the answer cannot be determined from the CONTEXT, is not within the general
  knowledge capabilities of the model, or requires updated information that the
  model cannot access, explicitly state the limitations and respond with
  </span><span class="sh">"</span><span class="s">I DO NOT KNOW</span><span class="sh">"</span><span class="s">.

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
</span><span class="sh">"""</span>
</code></pre></div></div> <p><strong>Key Design Principles:</strong></p> <ol> <li> <strong>Context-First Answering</strong> - Prioritizes user-specific documents over general knowledge</li> <li> <strong>Greeting Isolation</strong> - Explicitly prevents context injection for casual conversation</li> <li> <strong>Hybrid Knowledge</strong> - Allows general knowledge when context doesn‚Äôt cover the topic</li> <li> <strong>Boundary Awareness</strong> - Model understands when to use context vs. when to fall back</li> </ol> <h3 id="how-context-is-injected">How Context is Injected</h3> <p>The <code class="language-plaintext highlighter-rouge">{context}</code> placeholder is populated with the <strong>top-2 retrieved documents</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Retrieve top-2 most similar documents
</span><span class="n">top_2_docs</span> <span class="o">=</span> <span class="nf">cosine_similarity_search</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Format context as concatenated text
</span><span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">Document </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">top_2_docs</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Inject into prompt template
</span><span class="n">final_prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
    <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
    <span class="n">question</span><span class="o">=</span><span class="n">user_question</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="why-this-two-template-approach-works">Why This Two-Template Approach Works</h3> <table> <thead> <tr> <th>Scenario</th> <th>Template Used</th> <th>Rationale</th> </tr> </thead> <tbody> <tr> <td><strong>New User (no docs)</strong></td> <td>Empty Database Template</td> <td>Provides helpful responses while encouraging document uploads</td> </tr> <tr> <td><strong>Established User</strong></td> <td>Context Template</td> <td>Leverages personal documents for authentic, personalized responses</td> </tr> <tr> <td><strong>Greeting/Small Talk</strong></td> <td>Both (special handling)</td> <td>Maintains natural conversation flow without forcing context</td> </tr> </tbody> </table> <p><strong>Example Interaction Flow:</strong></p> <p><strong>Scenario 1: Empty Database</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User: "What's my favorite food?"
System: Uses Template 1
Model: "I do not have specific information about your preferences. Could you
       share some details about your favorite foods so I can learn more about you?"
</code></pre></div></div> <p><strong>Scenario 2: With Context</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Context (from top-2 docs):
- "Just had the best Thai curry ever! Spicy food is life üå∂Ô∏è"
- "Can't decide between pizza and tacos for dinner. Both are amazing!"

User: "What's my favorite food?"
System: Uses Template 2
Model: "Based on what you've shared, you love spicy food‚Äîespecially Thai curry!
       You also really enjoy pizza and tacos. Seems like you appreciate bold,
       flavorful dishes!"
</code></pre></div></div> <h3 id="behavioral-constraints">Behavioral Constraints</h3> <p>The prompts enforce critical constraints:</p> <ol> <li> <strong>No Hallucination</strong> - ‚ÄúI DO NOT KNOW‚Äù requirement prevents fabrication</li> <li> <strong>Contextual Awareness</strong> - Model knows when to use vs. ignore retrieved documents</li> <li> <strong>Tone Preservation</strong> - ‚ÄúFriendly and jovial‚Äù for greetings maintains personability</li> <li> <strong>Explicit Clarification</strong> - Requests more details instead of guessing</li> </ol> <p>This dual-template architecture allows PersonaAI to provide value <strong>immediately</strong> (even without documents) while becoming <strong>increasingly personalized</strong> as users upload more content.</p> <hr> <h2 id="llama-2-70b-model-selection">Llama 2 70B Model Selection</h2> <p><strong>Why Llama 2 70B?</strong></p> <p>PersonaAI uses <strong>Meta‚Äôs Llama 2 70B</strong> via the Replicate API for several critical reasons:</p> <h3 id="1-large-parameter-count-for-nuanced-understanding">1. Large Parameter Count for Nuanced Understanding</h3> <ul> <li> <strong>70 billion parameters</strong> provide sophisticated language understanding</li> <li>Captures subtle personality nuances in user documents</li> <li>Better context integration compared to smaller models</li> <li>Strong performance on instruction-following tasks</li> </ul> <h3 id="2-open-source-with-commercial-license">2. Open-Source with Commercial License</h3> <ul> <li> <strong>Free for commercial use</strong> (unlike GPT models with per-token charges)</li> <li>Transparent model architecture and training approach</li> <li>Community-driven improvements and optimizations</li> <li>Can self-host for complete data privacy if needed</li> </ul> <h3 id="3-optimized-for-instruction-following">3. Optimized for Instruction Following</h3> <p>Llama 2‚Äôs instruction-tuning makes it exceptionally responsive to structured prompts:</p> <ul> <li>Accurately interprets conditional logic in prompts (if-then clauses)</li> <li>Maintains behavioral constraints (e.g., ‚ÄúI DO NOT KNOW‚Äù requirement)</li> <li>Balances creativity with consistency</li> <li>Handles multi-paragraph context effectively</li> </ul> <h3 id="4-efficient-context-handling">4. Efficient Context Handling</h3> <ul> <li> <strong>4K token context window</strong> - Sufficient for top-2 retrieved documents + conversation history</li> <li>Optimized attention mechanisms for long-context tasks</li> <li>Fast inference via Replicate‚Äôs optimized infrastructure</li> </ul> <h3 id="model-configuration">Model Configuration</h3> <p><strong>Inference Parameters (via Replicate API):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">llama_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">meta/llama-2-70b-chat</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">system_prompt</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">You are a helpful and truthful assistant...</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>        <span class="c1"># Very low for deterministic, factual responses
</span>    <span class="sh">'</span><span class="s">top_p</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                 <span class="c1"># No nucleus sampling (use full distribution)
</span>    <span class="sh">'</span><span class="s">max_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>          <span class="c1"># Generous response length
</span>    <span class="sh">'</span><span class="s">repetition_penalty</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>    <span class="c1"># No explicit repetition penalty
</span><span class="p">}</span>
</code></pre></div></div> <p><strong>Why These Parameters?</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> <th>Rationale</th> </tr> </thead> <tbody> <tr> <td><strong>Temperature 0.05</strong></td> <td>Very low</td> <td>Prioritizes factual, consistent responses over creativity; reduces hallucination risk</td> </tr> <tr> <td><strong>Top-p 1</strong></td> <td>Full distribution</td> <td>No truncation; all tokens considered for sampling</td> </tr> <tr> <td><strong>Max Tokens 800</strong></td> <td>Long responses</td> <td>Allows detailed, comprehensive answers</td> </tr> <tr> <td><strong>Repetition Penalty 1</strong></td> <td>Neutral</td> <td>No artificial penalty; lets natural language flow guide repetition</td> </tr> </tbody> </table> <p><strong>Why Temperature 0.05?</strong></p> <p>The extremely low temperature is a <strong>deliberate design choice</strong> for PersonaAI:</p> <ol> <li> <strong>Factual Consistency</strong> - When answering from context, the model should extract information accurately, not creatively rephrase</li> <li> <strong>Reduces Hallucination</strong> - Lower temperature means higher probability tokens are heavily favored (more deterministic)</li> <li> <strong>Respects ‚ÄúI DO NOT KNOW‚Äù</strong> - Makes the model more likely to admit uncertainty rather than fabricate answers</li> <li> <strong>Predictable Behavior</strong> - Users expect consistent responses when asking the same question multiple times</li> </ol> <p><strong>Trade-off:</strong> Lower creativity in language generation, but this is acceptable since PersonaAI prioritizes <strong>authenticity</strong> (reflecting user documents) over <strong>novelty</strong>.</p> <hr> <h2 id="technical-implementation">Technical Implementation</h2> <h3 id="api-endpoints">API Endpoints</h3> <table> <thead> <tr> <th>Endpoint</th> <th>Method</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">/api/createuser/</code></td> <td>POST</td> <td>User registration with email/password</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">/api/token/</code></td> <td>POST</td> <td>JWT token generation for authentication</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">/api/token/refresh/</code></td> <td>POST</td> <td>Refresh expired access tokens</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">/api/</code></td> <td>GET/POST</td> <td>Document upload and retrieval</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">/api/prompt/</code></td> <td>POST</td> <td>Generate personalized AI responses</td> </tr> </tbody> </table> <h3 id="data-flow-example">Data Flow Example</h3> <p><strong>1. User uploads a document:</strong></p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">POST</span><span class="w"> </span><span class="err">/api/</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Just finished an amazing hike! The views were absolutely breathtaking..."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"user_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"user123"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p><strong>Response:</strong></p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"uploaded_at"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2025-01-15T10:30:00Z"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"topic"</span><span class="p">:</span><span class="w"> </span><span class="s2">"outdoor activities, nature"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"vector"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.234</span><span class="p">,</span><span class="w"> </span><span class="mf">-0.512</span><span class="p">,</span><span class="w"> </span><span class="mf">0.891</span><span class="p">,</span><span class="w"> </span><span class="err">...</span><span class="p">],</span><span class="w">  </span><span class="err">//</span><span class="w"> </span><span class="mi">768</span><span class="err">-dimensional</span><span class="w"> </span><span class="err">embedding</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Just finished an amazing hike! ..."</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p><strong>2. User sends a prompt:</strong></p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">POST</span><span class="w"> </span><span class="err">/api/prompt/</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"> </span><span class="s2">"What should I do this weekend?"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"user_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"user123"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p><strong>Internal Processing:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Generate query embedding
</span><span class="n">query_vec</span> <span class="o">=</span> <span class="nf">embed</span><span class="p">(</span><span class="sh">"</span><span class="s">What should I do this weekend?</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. Retrieve top-2 similar documents via cosine similarity
</span><span class="n">top_2_docs</span> <span class="o">=</span> <span class="nf">cosine_similarity_search</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># 3. Format context from top-2 documents
</span><span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">doc</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">top_2_docs</span><span class="p">])</span>

<span class="c1"># 4. Select appropriate prompt template
</span><span class="k">if</span> <span class="n">user_has_documents</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">context_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">question</span><span class="o">=</span><span class="n">user_query</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">empty_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">user_query</span><span class="p">)</span>

<span class="c1"># 5. Generate response with Llama 2 70B via Replicate
</span><span class="n">response</span> <span class="o">=</span> <span class="n">replicate</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">meta/llama-2-70b-chat</span><span class="sh">"</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">system_prompt</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful and truthful assistant...</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">repetition_penalty</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <p><strong>Response:</strong></p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"response"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Based on your recent activities, you should definitely go for another hike if the weather permits! You mentioned loving outdoor adventures and finding them rejuvenating. Alternatively, if you prefer something more relaxed, you could enjoy a good book with your favorite coffee blend‚Äîyou've mentioned both as weekend favorites."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"retrieved_docs"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Just finished an amazing hike! ..."</span><span class="p">,</span><span class="w"> </span><span class="nl">"similarity"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.89</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Love weekends where I can just..."</span><span class="p">,</span><span class="w"> </span><span class="nl">"similarity"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.82</span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <hr> <h2 id="technologies-used">Technologies Used</h2> <div class="row"> <div class="col-sm-6"> **Backend** - Django 4.x - Django REST Framework - JWT Authentication - Python 3.10+ </div> <div class="col-sm-6"> **AI/ML Stack** - Llama 2 70B (via Replicate API) - Sentence Transformers (embeddings) - NumPy (cosine similarity) - Docker (deployment) </div> </div> <hr> <h2 id="privacy--security">Privacy &amp; Security</h2> <p>PersonaAI prioritizes user data protection:</p> <ol> <li> <strong>End-to-End Encryption</strong> - Documents encrypted at rest and in transit</li> <li> <strong>User Isolation</strong> - Strict database-level separation per user</li> <li> <strong>Local Deployment Option</strong> - Self-host Llama 2 to avoid third-party data sharing</li> <li> <strong>JWT Authentication</strong> - Secure, stateless authentication with token expiration</li> <li> <strong>No Training Data Leakage</strong> - RAG retrieval doesn‚Äôt modify the base Llama 2 model; only context injection at inference time</li> </ol> <hr> <h2 id="results--impact">Results &amp; Impact</h2> <h3 id="applications">Applications</h3> <ol> <li> <strong>Personal AI Assistants</strong> - Digital doubles for busy professionals</li> <li> <strong>Legacy Preservation</strong> - Immortalize communication styles of loved ones</li> <li> <strong>Content Creation</strong> - Generate social media posts in authentic personal voice</li> <li> <strong>Customer Service</strong> - Brand-consistent chatbots trained on company communication</li> <li> <strong>Research</strong> - Study personality expression in language models</li> </ol> <h3 id="performance-metrics">Performance Metrics</h3> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><strong>Retrieval Speed</strong></td> <td>&lt;50ms for top-2 @ 10K documents</td> </tr> <tr> <td><strong>Inference Latency</strong></td> <td>~3-5s (Llama 2 70B via Replicate)</td> </tr> <tr> <td><strong>Context Efficiency</strong></td> <td>Top-2 retrieval minimizes token usage</td> </tr> <tr> <td><strong>Privacy Compliance</strong></td> <td>GDPR/CCPA compliant with local deployment option</td> </tr> </tbody> </table> <hr> <h2 id="source-code">Source Code</h2> <div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center"> <a href="https://github.com/Kunle-xy/ai-personas" target="_blank" rel="external nofollow noopener"> <img src="https://gh-card.dev/repos/Kunle-xy/ai-personas.svg" alt="GitHub Repository"> </a> </div> <p><strong>Quick Start:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/Kunle-xy/ai-personas.git
<span class="nb">cd </span>ai-personas
docker-compose up <span class="nt">-d</span>
</code></pre></div></div> <p>Access the application at <code class="language-plaintext highlighter-rouge">http://localhost:8000</code></p> <hr> <h2 id="related-publication">Related Publication</h2> <p>This project is documented in the research paper:</p> <p><strong>PersonaAI: A Cutting-Edge Application Leveraging RAG and LLAMA</strong> Available at: <a href="https://arxiv.org/abs/2503.15489" rel="external nofollow noopener" target="_blank">arXiv:2503.15489</a></p> <hr> <h2 id="future-enhancements">Future Enhancements</h2> <ol> <li> <strong>Multimodal Personas</strong> - Incorporate images, voice recordings, and video</li> <li> <strong>Temporal Adaptation</strong> - Track personality evolution over time</li> <li> <strong>Multi-Agent Conversations</strong> - Enable dialogue between different persona instances</li> <li> <strong>Fine-Tuning Integration</strong> - Combine RAG with LoRA fine-tuning for hybrid approach</li> <li> <strong>Emotion Detection</strong> - Analyze and replicate emotional patterns in responses</li> </ol> <hr> <h2 id="technical-skills-demonstrated">Technical Skills Demonstrated</h2> <ul> <li>Retrieval-Augmented Generation (RAG) architecture</li> <li>Vector embeddings and cosine similarity search</li> <li>Dual-template prompt engineering with conditional logic</li> <li>Llama 2 70B deployment via Replicate API</li> <li>Django REST API development</li> <li>JWT authentication and security</li> <li>Docker containerization</li> <li>Privacy-preserving AI system design</li> <li>Top-k retrieval optimization (k=2 strategy)</li> </ul> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Kunle Sunday Oguntoye. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>